{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c1d62-a362-4551-bb74-1f8ad192b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import datetime\n",
    "from typing import Dict\n",
    "import os\n",
    "import json\n",
    "from langchain.llms import Ollama\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68ce0d",
   "metadata": {},
   "source": [
    "## OBJECT DETECTION (PERSON)\n",
    "#### YOLOV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821d775-5ed6-44a5-8c71-04af30fe4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch \n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the yolov5 directory to path\n",
    "sys.path.append(r\"C:\\Users\\ASUS\\Women Safety Ecosystem\\yolov5\\models\\yolov5m.yaml\")  \n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.classes = [0]  # Only detect persons (class 0 in COCO dataset)\n",
    "model.conf = 0.5  # Confidence threshold\n",
    "model.cpu()  # Ensure model runs on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a44a6-b13a-4430-a60e-c580f2db1e6b",
   "metadata": {},
   "source": [
    "# Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611ef14-4f55-4144-b40b-692bdf7b0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gender classification model\n",
    "gender_model = load_model('cctv_gender_classifier.h5')\n",
    "gender_classes = ['man', 'woman']\n",
    "\n",
    "# Initialize DeepSort\n",
    "tracker = DeepSort(max_age=10, \n",
    "                   n_init=3,\n",
    "                   nms_max_overlap=1.0,\n",
    "                   max_cosine_distance=0.3,\n",
    "                   nn_budget=None,\n",
    "                   override_track_class=None,\n",
    "                   embedder=\"mobilenet\",\n",
    "                   half=True,\n",
    "                   bgr=True,\n",
    "                   embedder_gpu=False,\n",
    "                   embedder_model_name=None,\n",
    "                   embedder_wts=None,\n",
    "                   polygon=False,\n",
    "                   today=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the action recognition pipeline\n",
    "action_pipe = pipeline(\"image-classification\", model=\"rvv-karma/Human-Action-Recognition-VIT-Base-patch16-224\", framework=\"tf\")\n",
    "\n",
    "def preprocess_body(body_crop, target_size=(126, 126)):\n",
    "    try:\n",
    "        body_crop = cv2.resize(body_crop, target_size)\n",
    "        body_crop = body_crop.astype(\"float32\") / 255.0\n",
    "        body_crop = img_to_array(body_crop)\n",
    "        body_crop = np.expand_dims(body_crop, axis=0)\n",
    "        return body_crop\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_body: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2aee4-66cf-4316-af65-60053a0d098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "total_persons = 0\n",
    "gender_counts = {'man': 0, 'woman': 0}\n",
    "frame_skip = 2\n",
    "processing_times = []\n",
    "fighting_count = 0\n",
    "lone_woman_flag = False\n",
    "surrounded_woman_flag = False\n",
    "\n",
    "# New variables for report generation\n",
    "sos_events = []\n",
    "alerts = []\n",
    "warnings = []\n",
    "gender_counts_over_time = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd660c-9255-4d18-9ead-e2446d1c9d93",
   "metadata": {},
   "source": [
    "# CCTV Surveillance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4138793-33d1-4e59-8f9f-a66a946c49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(r\"C:\\Users\\ASUS\\Desktop\\Important files\\Women safety ew\\Zinnovation\\SIH Videos\\WhatsApp Video 2024-12-04 at 11.27.37_9177274c.mp4\")\n",
    "frame_count = 0\n",
    "\n",
    "# Variables for tracking and detection\n",
    "frame_skip = 2  # Adjust based on performance needs\n",
    "total_persons = 0\n",
    "alerts = []\n",
    "warnings = []\n",
    "sos_events = []\n",
    "gender_counts_over_time = []\n",
    "\n",
    "# Variables for SOS recording\n",
    "sos_recording = False\n",
    "output_video = None\n",
    "output_filename = \"\"\n",
    "\n",
    "# Create a folder to store SOS recordings \n",
    "sos_folder = \"SOS_Recordings\"\n",
    "if not os.path.exists(sos_folder):\n",
    "    os.makedirs(sos_folder)\n",
    "\n",
    "# Detection and tracking parameters\n",
    "CONFIDENCE_THRESHOLD = 0.25\n",
    "NMS_THRESHOLD = 0.3\n",
    "\n",
    "# Initialize DeepSort with default parameters\n",
    "tracker = DeepSort()  # Using default configuration\n",
    "\n",
    "def is_night(hour):\n",
    "    return hour < 6 or hour >= 18\n",
    "\n",
    "def print_alert(message):\n",
    "    current_time = datetime.datetime.now()\n",
    "    alert_info = {\n",
    "        \"message\": message,\n",
    "        \"date\": current_time.strftime('%Y-%m-%d'),\n",
    "        \"time\": current_time.strftime('%I:%M:%S %p'),\n",
    "        \"location\": \"XYZ\"\n",
    "    }\n",
    "    alerts.append(alert_info)\n",
    "    print(f\"Alert: {message}\")\n",
    "    print(f\"Date: {alert_info['date']}\")\n",
    "    print(f\"Time: {alert_info['time']}\")\n",
    "    print(f\"Location: {alert_info['location']}\")\n",
    "    print(\"-------------------\")\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "    \n",
    "    # Resize frame for faster processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    # Get current date and time\n",
    "    current_time = datetime.datetime.now()\n",
    "    time_str = current_time.strftime(\"%Y-%m-%d %I:%M:%S %p\")\n",
    "    cv2.putText(frame, time_str, (320, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    cv2.putText(frame, time_str, (320, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # Determine if it's day or night\n",
    "    is_night_time = is_night(current_time.hour)\n",
    "    day_night_str = \"Night\" if is_night_time else \"Day\"\n",
    "    cv2.putText(frame, day_night_str, (10, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    cv2.putText(frame, day_night_str, (10, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # YOLOv5 detection\n",
    "    results = model(frame)\n",
    "    detections = results.xyxy[0].cpu().numpy()\n",
    "\n",
    "    # Apply Non-Maximum Suppression\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    filtered_detections = []\n",
    "    \n",
    "    for det in detections:\n",
    "        if det[4] >= CONFIDENCE_THRESHOLD:\n",
    "            boxes.append([det[0], det[1], det[2] - det[0], det[3] - det[1]])\n",
    "            scores.append(det[4])\n",
    "            filtered_detections.append(det)\n",
    "    \n",
    "    if boxes:\n",
    "        indices = cv2.dnn.NMSBoxes(\n",
    "            boxes,\n",
    "            scores,\n",
    "            CONFIDENCE_THRESHOLD,\n",
    "            NMS_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Prepare filtered detections for DeepSort\n",
    "        deepsort_detections = []\n",
    "        for idx in indices:\n",
    "            det = filtered_detections[idx]\n",
    "            x1, y1, x2, y2, conf, cls = det\n",
    "            deepsort_detections.append(([x1, y1, x2 - x1, y2 - y1], conf, int(cls)))\n",
    "    \n",
    "        # Update tracks with filtered detections\n",
    "        tracks = tracker.update_tracks(deepsort_detections, frame=frame)\n",
    "    else:\n",
    "        # Update without detections if none were found\n",
    "        tracks = tracker.update_tracks([], frame=frame)\n",
    "    \n",
    "    # Reset gender counts for each frame\n",
    "    gender_counts = {'man': 0, 'woman': 0}\n",
    "    \n",
    "    # Process tracks\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        \n",
    "        # Initialize gender history if not present\n",
    "        if not hasattr(track, 'gender_history'):\n",
    "            track.gender_history = []\n",
    "        \n",
    "        # Perform gender classification with temporal smoothing\n",
    "        if len(track.gender_history) < 5:\n",
    "            body_crop = frame[y1:y2, x1:x2]\n",
    "            preprocessed_body = preprocess_body(body_crop)\n",
    "            if preprocessed_body is not None:\n",
    "                try:\n",
    "                    gender_conf = gender_model.predict(preprocessed_body)[0]\n",
    "                    gender_idx = np.argmax(gender_conf)\n",
    "                    if gender_conf[gender_idx] > 0.7:\n",
    "                        track.gender_history.append(gender_classes[gender_idx])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in gender classification: {e}\")\n",
    "        \n",
    "        # Use majority voting for stable gender classification\n",
    "        if track.gender_history:\n",
    "            track.gender = max(set(track.gender_history), key=track.gender_history.count)\n",
    "        \n",
    "        # Update gender counts and draw bounding box\n",
    "        if hasattr(track, 'gender'):\n",
    "            gender_counts[track.gender] += 1\n",
    "        \n",
    "        # Draw box with different colors based on tracking status\n",
    "        color = (0, 255, 0) if track.time_since_update == 0 else (0, 165, 255)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        label = f\"ID {track_id}\"\n",
    "        if hasattr(track, 'gender'):\n",
    "            label += f\" ({track.gender})\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # Perform action recognition\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    action_result = action_pipe(pil_image)\n",
    "    \n",
    "    # Check for fighting action\n",
    "    sos_detected = False\n",
    "    for item in action_result:\n",
    "        if item['label'] == 'Fighting' and item['score'] > 0.92:\n",
    "            sos_detected = True\n",
    "            sos_event = {\n",
    "                \"type\": \"Fighting\",\n",
    "                \"timestamp\": current_time.strftime(\"%Y-%m-%d %I:%M:%S %p\"),\n",
    "                \"location\": \"XYZ\",\n",
    "                \"men_count\": gender_counts['man'],\n",
    "                \"women_count\": gender_counts['woman']\n",
    "            }\n",
    "            sos_events.append(sos_event)\n",
    "            print_alert(\"SOS! Fighting detected. Action must be taken.\")\n",
    "            cv2.putText(frame, \"ALERT! SOS!!\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "            if not sos_recording:\n",
    "                sos_recording = True\n",
    "                output_filename = os.path.join(sos_folder, f\"{current_time.strftime('%d-%m-%Y _ %H:%M')}@XYZ.mp4\")\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                output_video = cv2.VideoWriter(output_filename, fourcc, 30.0, (640, 480))\n",
    "            break\n",
    "\n",
    "    # Update and display warnings\n",
    "    lone_woman_flag = gender_counts['woman'] == 1 and gender_counts['man'] == 0\n",
    "    surrounded_woman_flag = gender_counts['woman'] == 1 and gender_counts['man'] >= 2\n",
    "\n",
    "    if not sos_detected:\n",
    "        if lone_woman_flag:\n",
    "            warning_msg = \"Warning: Lone woman at night\" if is_night_time else \"Notice: Lone woman at daytime\"\n",
    "            cv2.putText(frame, warning_msg, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            warnings.append({\n",
    "                \"message\": warning_msg,\n",
    "                \"timestamp\": current_time.strftime(\"%Y-%m-%d %I:%M:%S %p\"),\n",
    "            })\n",
    "            print_alert(f\"{warning_msg}. There is potential threat.\")\n",
    "        elif surrounded_woman_flag:\n",
    "            warning_msg = \"Warning: Woman surrounded by men\" \n",
    "            cv2.putText(frame, warning_msg, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "            warnings.append({\n",
    "                \"message\": warning_msg,\n",
    "                \"timestamp\": current_time.strftime(\"%Y-%m-%d %I:%M:%S %p\"),\n",
    "                \"location\": \"XYZ\"\n",
    "            })\n",
    "            print_alert(f\"{warning_msg}. There is potential threat.\")\n",
    "\n",
    "    # Record frame if SOS is active\n",
    "    if sos_recording and output_video is not None:\n",
    "        output_video.write(frame)\n",
    "\n",
    "    # Display stats and store counts\n",
    "    cv2.putText(frame, f\"Men: {gender_counts['man']}, Women: {gender_counts['woman']}\", \n",
    "                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    gender_counts_over_time.append({\n",
    "        \"timestamp\": current_time.strftime(\"%Y-%m-%d %I:%M:%S %p\"),\n",
    "        \"men_count\": gender_counts['man'],\n",
    "        \"women_count\": gender_counts['woman']\n",
    "    })\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Full Body Gender Detection and Action Recognition', frame)\n",
    "    0\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67450c89-0a42-4486-b510-455294b8273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release resources\n",
    "video.release()\n",
    "if output_video is not None:\n",
    "    output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "if sos_recording:\n",
    "    print(f\"SOS video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = [\n",
    "    {\n",
    "        \"frame_number\": 1,\n",
    "        \"detections\": [\n",
    "            {\"tracking_id\": 1, \"gender\": \"male\"},\n",
    "            {\"tracking_id\": 2, \"gender\": \"female\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"frame_number\": 2,\n",
    "        \"detections\": [\n",
    "            {\"tracking_id\": 1, \"gender\": \"male\"},  # Same male as frame 1\n",
    "            {\"tracking_id\": 3, \"gender\": \"female\"}  # New female\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"frame_number\": 3,\n",
    "        \"detections\": [\n",
    "            {\"tracking_id\": 2, \"gender\": \"female\"},  # Same female as frame 1\n",
    "            {\"tracking_id\": 3, \"gender\": \"female\"},  # Same female as frame 2\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Initialize sets to store unique IDs\n",
    "unique_male_ids = set()\n",
    "unique_female_ids = set()\n",
    "\n",
    "# Initialize SOS, alerts, and warnings data\n",
    "sos_events = [{\"timestamp\": \"2024-12-06 12:00:00 PM\"}, {\"timestamp\": \"2024-12-06 12:10:00 PM\"}]\n",
    "alerts = []\n",
    "warnings = [{\"timestamp\": \"2024-12-06 12:20:00 PM\"}, {\"timestamp\": \"2024-12-06 12:30:00 PM\"}, {\"timestamp\": \"2024-12-06 12:40:00 PM\"}]\n",
    "\n",
    "# Aggregation function\n",
    "def aggregate_incidents(incident_list, threshold):\n",
    "    \"\"\"\n",
    "    Aggregates incidents within a threshold time interval.\n",
    "    \"\"\"\n",
    "    if not incident_list:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        incident_times = [datetime.datetime.strptime(event['timestamp'], \"%Y-%m-%d %I:%M:%S %p\") for event in incident_list]\n",
    "        incident_times.sort()\n",
    "\n",
    "        aggregated_count = 1\n",
    "        for i in range(1, len(incident_times)):\n",
    "            if (incident_times[i] - incident_times[i - 1]).total_seconds() / 60 > threshold:\n",
    "                aggregated_count += 1\n",
    "\n",
    "        return aggregated_count\n",
    "    except KeyError as e:\n",
    "        print(f\"No valid incidents found with '{e.args[0]}'\")\n",
    "        return 0\n",
    "\n",
    "# Process each frame to track unique individuals\n",
    "for frame in video_frames:\n",
    "    for person in frame['detections']:\n",
    "        tracking_id = person['tracking_id']\n",
    "        gender = person['gender']\n",
    "        \n",
    "        if gender == \"female\":\n",
    "            unique_male_ids.add(tracking_id)\n",
    "        elif gender == \"male\":\n",
    "            unique_female_ids.add(tracking_id)\n",
    "\n",
    "# Aggregate SOS events, alerts, and warnings\n",
    "aggregated_sos_events = aggregate_incidents(sos_events, threshold=10)\n",
    "aggregated_alerts = aggregate_incidents(alerts, threshold=10)\n",
    "aggregated_warnings = aggregate_incidents(warnings, threshold=10)\n",
    "\n",
    "# Final counts of unique individuals\n",
    "total_unique_men = len(unique_male_ids)\n",
    "total_unique_women = len(unique_female_ids)\n",
    "total_unique_persons = total_unique_men + total_unique_women\n",
    "\n",
    "# Generate the updated summary\n",
    "print(\"======== Video Analysis Summary ========\")\n",
    "print(f\"Aggregated SOS events: {aggregated_sos_events}\")\n",
    "print(f\"Aggregated warnings: {aggregated_warnings}\")\n",
    "print(f\"Total unique persons: {total_unique_persons}\")\n",
    "print(f\"Men: {total_unique_men}\") \n",
    "print(f\"Women: {total_unique_women}\") \n",
    "print(\"Location: XYZ\")\n",
    "print(f\"Gender counts recorded over time: {len(video_frames)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5564329-69c3-4f83-9faa-50a4d93639fd",
   "metadata": {},
   "source": [
    "# REPORT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a487bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "from typing import Dict\n",
    "\n",
    "def generate_security_report(\n",
    "    total_unique_persons: int,\n",
    "    total_unique_men: int,\n",
    "    total_unique_women: int,\n",
    "    aggregated_sos_events: int,\n",
    "    aggregated_warnings: int,\n",
    "    frame_count: int,\n",
    "    location: str = \"XYZ\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a security report using the Ollama Gemma model based on surveillance data.\n",
    "    \n",
    "    Args:\n",
    "        total_unique_persons: Total number of unique individuals detected\n",
    "        total_unique_men: Number of unique men detected\n",
    "        total_unique_women: Number of unique women detected\n",
    "        aggregated_sos_events: Number of aggregated SOS events\n",
    "        aggregated_warnings: Number of aggregated warnings\n",
    "        frame_count: Number of frames analyzed\n",
    "        location: Location of surveillance\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated security report\n",
    "    \"\"\"\n",
    "    \n",
    "    # Structure the data for the prompt\n",
    "    summary_data = {\n",
    "        \"surveillance_summary\": {\n",
    "            \"total_frames_analyzed\": frame_count,\n",
    "            \"unique_persons\": {\n",
    "                \"total\": total_unique_persons,\n",
    "                \"men\": total_unique_men,\n",
    "                \"women\": total_unique_women\n",
    "            },\n",
    "            \"incidents\": {\n",
    "                \"sos_events\": aggregated_sos_events,\n",
    "                \"warnings\": aggregated_warnings\n",
    "            },\n",
    "            \"location\": location\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = f\"\"\"As a security analysis system, generate a concise but detailed report based on the following surveillance data:\n",
    "{json.dumps(summary_data, indent=2)}\n",
    "Generate a professional report covering:\n",
    "1. Key statistics (total persons detected, gender distribution)\n",
    "2. Security incidents (SOS events and warnings)\n",
    "3. Notable patterns or concerns\n",
    "4. Recommendations based on the findings\n",
    "Format the report in clear, concise paragraphs suitable for security personnel.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Prepare the request to Ollama API\n",
    "        url = 'http://localhost:11434/api/chat'\n",
    "        payload = {\n",
    "            \"model\": \"gemma2:2b\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        # Send request to local Ollama server\n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Extract the report text from the response\n",
    "            report = response.json()['message']['content']\n",
    "            return report\n",
    "        else:\n",
    "            raise ConnectionError(f\"API request failed with status code {response.status_code}\")\n",
    "    \n",
    "    except ConnectionError as e:\n",
    "        print(\"\\nConnection Error: Please ensure Ollama is running locally with these steps:\")\n",
    "        print(\"1. Install Ollama from: https://ollama.ai/\")\n",
    "        print(\"2. Run 'ollama serve' in terminal\")\n",
    "        print(\"3. In another terminal, run 'ollama pull gemma:2b'\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError generating report: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def save_report(report: str, prefix: str = \"surveillance_report\") -> str:\n",
    "    \"\"\"Save the generated report to a file with timestamp.\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{prefix}_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Using dynamic summary data from video analysis\n",
    "        total_unique_men = len(unique_male_ids)  # From your video analysis data\n",
    "        total_unique_women = len(unique_female_ids)  # From your video analysis data\n",
    "        total_unique_persons = total_unique_men + total_unique_women  # From your video analysis data\n",
    "        aggregated_sos_events = aggregated_sos_events  # From your video analysis data\n",
    "        aggregated_warnings = aggregated_warnings  # From your video analysis data\n",
    "        frame_count = len(video_frames)  # Number of frames in the video analysis\n",
    "        location = \"XYZ\"  # Update with location from your system, if needed\n",
    "        \n",
    "        # Generate the report using dynamic data\n",
    "        report = generate_security_report(\n",
    "            total_unique_persons=total_unique_persons,\n",
    "            total_unique_men=total_unique_men,\n",
    "            total_unique_women=total_unique_women,\n",
    "            aggregated_sos_events=aggregated_sos_events,\n",
    "            aggregated_warnings=aggregated_warnings,\n",
    "            frame_count=frame_count,\n",
    "            location=location\n",
    "        )\n",
    "        \n",
    "        # Save and display the report\n",
    "        filename = save_report(report)\n",
    "        \n",
    "        print(\"\\nGenerated Report:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(report)\n",
    "        print(\"\\nReport saved to:\", filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failed to generate report:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829412f-185d-4633-b612-f780da06a3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f4aa5-9ffa-469e-bb4e-4c69aca1fda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172671f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a64eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (yolov5_env)",
   "language": "python",
   "name": "yolov5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
